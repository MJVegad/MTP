\documentclass[a4paper,11pt]{report}


%\usepackage[a4paper,left=2.4cm,right=2.4cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage [a4paper,total={6in,10in}]{geometry}
%\usepackage[nottoc,notlot,notlof]{tocbibind}
\addcontentsline{toc}{chapter}{References}
\usepackage{float}
\restylefloat{table}
\usepackage{array}
\newcolumntype{L}[1]{>{\raggedright\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{C}[1]{>{\centering\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{R}[1]{>{\raggedleft\let\newline\\\arraybackslash\hspace{0pt}}m{#1}}
\newcolumntype{Z}[1]{%
 >{\vbox to 5ex\bgroup\vfill\centering}%
 p{#1}%
 <{\egroup}}  
 \usepackage{indentfirst}
\graphicspath{{../}}
\renewcommand{\bibname}{References}


\title{\textbf { \vspace{3pt} A {\textbf M}iddlebox {\textbf s}pecialized {\textbf H}ypervisor}} 
\author{\vspace{2cm} A project report \vspace{3cm} Submitted in partial fulfillment of requirements for the degree of \vspace{2cm} Master of Technology \vspace{2cm} By \vspace{2cm} \textbf{Mihir Vegad J.} \vspace{1cm} 143050073 \vspace{2cm} under the guidance of \vspace{2cm} \textbf{Prof. Purushottam Kulkarni} \vspace{2cm}  
  } 



\begin{document}


%\date{}
%\maketitle


%\begin{figure}[h]
%\centering
%\includegraphics[scale=0.3]{iitb.png}
%\end{figure} 
%\vspace{3cm}
%{\center{Department of Computer Science and Engineering \\ Indian Institute of Technology, %Bombay \vspace{1cm}}} 


\begin{titlepage}
\begin{center}

\vspace*{2cm}

%\textsc{\fontsize{20}{24}\selectfont Indian Institute of Technology, Bombay}\\[2cm]
\textsc{\Large \bf Project Report}\\[0.85cm]

\hrulefill
\\[1cm]
{\huge \bf A {\textbf M}iddlebox {\textbf s}pecialized {\textbf H}ypervisor}\\[0.4cm]
\hrulefill
\\[1cm]

\begin{minipage}{0.44\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
{\Large \textbf {Mihir J. Vegad}}
\end{flushleft}
\end{minipage}
\begin{minipage}{0.44\textwidth}
\begin{flushright} \large
\emph{Guide:} \\
{\Large \textbf {Prof. Purushottam Kulkarni}}
\end{flushright}
\end{minipage}\\[2cm]

\large \textit{A report submitted in partial fulfilment of the requirements\\
[0.5cm] for the degree of Master of Technology in the\\ 
[0.5cm]Computer Science and Engineering}\\[2cm]

\includegraphics[width=4cm]{iitb.png}\\[0.75cm]

\textsc{\large Department of Computer Science and Engineering\\Indian Institute of Technology, Bombay}
\\[1cm]
%{\large \mydate\today} % Date
%\includegraphics{Logo} % University/department logo - uncomment to place it

\end{center}
\end{titlepage}
\clearpage

\newpage
\vspace*{3cm}
{\center \textbf {Acknowledgement}\\}

\vspace{0.5cm}
\noindent I would like to thank my guide, \textbf {Prof. Purushottam Kulkarni} for giving me the opportunity to work in this field. I really appreciate the efforts which he put in throughout the project, to understand the work done by us and then to guide us to the next step. During this process, I learned a lot and overall it has created strong base for me in the field of NFV/SDN/Virtualization. I would also like to thank fellow SYNERG mates for extending their support whenever it was required.   
\newpage

\vspace*{2cm}
{\center \textbf {Abstract}\\}
\vspace{1cm}
\noindent    

%\noindent Virtualization has started becoming a boom in the market as corporate world wants to do more with less resources. Virtualization provides benefits like improvement of  overall infrastructure cost, overall management cost and eliminates the hardware vendor dependancy. We will discuss a type of virtualization which simplifies the deployment and management of network functions, that is Network Function Virtualization. NFV virtualizes the dedicated hardware network functions to a software network application which can run on standard x86 machines. These network functions are called middleboxes. We will discuss presence of the middleboxes in current data centres and why it is important to optimize the performance of middleboxes. When we talk about virtualization, hypervisor becomes the name of the game. We will define the term hypervisor and explain its role in virtualization in more detail. There are many techniques to host middleboxes on the physical machines in data centre networks. All the techniques have their own ups and downs.                             
%We have come up with our own technique to efficiently host middleboxes on the physical machine. We named it as Mypervisor that is middlebox specialised hypervisor. Based on our study of middleboxes and hypervisors, we concluded that middlebox performance in data centres can still be optimized. Mypervisor offloads some middlebox functionality to the hypervisor to meet performance requirements. We proposed a design for Mypervisor and implemented it for a Wire middlebox. Wire Mypervisor will prove to be a stepping stone for implementation of a more complete Mypervisor.                

\newpage
\tableofcontents
\listoffigures
%\begingroup
%\let\clearpage\relax
\listoftables
%\endgroup
\newpage
%\bigskip
\chapter{Introduction}
\noindent {One of the key factor in the rise of the Data center networking in recent years is, the core
system infrastructure such as storage, network devices has become software-defined. Network Function Virtualization decouples the network applications from proprietary hardware and virtualize them. These network functions are usually referred as Middleboxes. To run middleboxes on virtual machines, we require a Virtual Machine Monitor or Hypervisor which allow multiple commodity virtual machines to share conventional hardware in a safe and resource managed manner. In the era of virtualization and Software Defined Networking, Data centres are being flooded with various middleboxes and we want to virtualize them efficiently without compromising the overall performance or resource utilization.}
\section{Middleboxes and Hypervisors}
By definition a middlebox is a networking function that transforms, inspects, filters or manipulates network traffic for some purpose other than packet forwarding.~\cite{MB} NFV has attracted many service providers to virtualize their network. Virtualized network functions would be located in those data centres along with other network nodes. According to a survey, number of middleboxes deployed in varying size data centres are on par with number of L3 routers and number of L2 switches.~\cite{DM} Wan optimizers, proxies, application layer gateways, load balancers, intrusion detection system, intrusion prevention system are widely used middleboxes in data centres. Middleboxes play an important role in meeting service level agreements(SLAs) with client. Traffic is normally routed through chain of such middleboxes. All the middleboxes perform some entry level task followed by specific middlebox function followed by packet forwarding. They perform some redundant tasks which increases packet processing time through a middlebox chain and overall response time to user. To deal with this issue we have to leverage virtualization as in the transition from traditional network, hypervisor is still one component which operates in traditional manner.                        

The sole aim of virtualization is to run many virtual systems on a single physical system efficiently. Hypervisors are softwares/firmwares that creates or destroys virtual machines and manages physical resources among them. Two different types of hypervisors, Bare metal hypervisor and Hosted hypervisors are widely used in the market. Bare metal hypervisor runs directly on the hardware and manages virtual machines. Hosted hypervisor runs as an application on the operating system, usually known as host operating system. Oracle VM Server for x86, Xen server, KVM, Hyper-v are examples of bare metal hypervisors. VmWare workstation/player, virtual box are example of hosted hypervisors. We will focus on Bare metal hypervisors (Type-1) as they are widely used in data centres. A hypervisor mainly provides virtual hard disk, virtual network interface card, and life cycle management facility to virtual machines. We will discuss what more a hypervisor can offer to middleboxes, apart from this traditional stuff. 
\begin{figure}[h]
\centering
\includegraphics[scale = 0.8]{hyp.pdf}
\caption{Different types of Hypervisor}
\end{figure}  
Mostly used hypervisors in data centre virtualization are vSphere from VMware, Xenserver from Citrix, hyper-v from Microsoft and KVM from Redhat. Different hypervisors are used as per the requirements of the data centre administrator. We will focus on Kernel Virtual Machine(KVM) as a hypervisor in our experiments. KVM virtualizes the processor and the memory of the system. QEMU is a 'Quick Emulator' which is used to arbitrate hardware resources such as disk and network interface card. QEMU is a type-2 hypervisor by itself, it executes the virtual CPU instruction using host operating system on the physical cpu. But when QEMU combines with KVM, the combination becomes type-1 hypervisor. In that combination, QEMU is used as a virtualizer, and it achieves near native performance by executing guests code directly on hardware. We have used virtio enabled KVM setup for experiments. Virtio is a standard for network device driver while transmitting or receiving a packet from/to that network device. In this case, network device driver just know that it is running in the virtualized environment.

When a packet is received on the physical network interface card, it is hypervisor's duty to forward it to the specific middlebox hosted on that machine or vice versa. In our KVM setup, how this packet travels from the physical NIC to middlebox or viceversa is very important to understand before we move towards design of the solution. When a packet is received on pNIC, It is forwarded to the hypervisor bridge. Bridge forwards it to the tap interface for destination middlebox. Tap interface forwards it to vNIC. And then at last it is received by the middlebox. This is very brief description of the whole process. We will discuss this in more detail in chapter 5.
%\section{Traditional Hypervisors}
\section{Problem Statement}        
%\section{Problem Statement}
\section{Organization of the work}  
%\section{Outline of the report}    

\chapter{NFV in Data centers: Middleboxes}
\section{Why to focus on MB?}
\noindent Middleboxes are crucial part of data centre networks as only packet forwarding is not good enough to meet customer requirements, other functionalities like Quality of Service/Quality of Experience, load balancing, security are needed to make customer experience complete. This fact is also supported by a survey done over 57 enterprise data centres, whose size varied from less than 1K hosts to more than 100K hosts.~\cite{DM} The survey shows that number of middleboxes deployed in data centres are on par with number of routers and number of switches in the data centre.  
\begin{figure}[h]
\centering
\includegraphics[scale = 0.5]{g1.pdf}
\caption{MB presence in the data centres~\cite{DM}}
\end{figure} 
\section{Middlebox examples}
\subsection{General middleboxes}
Some of the widely used middleboxes among the data centres are described here.
\begin{itemize}
\item \textbf{Firewalls: }A firewall is a network security system that monitors and controls the incoming and outgoing traffic based on predetermined security rules set by administrator.~\cite{FW}
\item \textbf{IDS/IPS: }IDS is a software application that monitors the network traffic for malicious activities or policy violation and produces a report to administrator. IPS is proactive approach of monitoring network traffic and identifying malicious activities and prevent them from occuring.~\cite{IDS}
\item \textbf{Load Balancer: }Load balancer distributes the incoming requests among a set of resources available. Efficient load balancer leads to optimized resource utilization, maximized throughput and minimized response time.
\item \textbf{Wan optimizer: }It improves bandwidth consumption and latency between dedicated end points. It coordinates to cache and compress traffic that traverses the internet.~\cite{MB}
\item \textbf{Network Address Translators: }It serves network traffic to multiple private machines which are exposed to internet through a public host. It modifies the 4-tuple address fields of the packets to ensure that it reaches to the correct host.
\item \textbf{Traffic shaper: }Traffic shaper is also known as a rate limiter. It limits the amount of bandwidth for specific traffic.
\item \textbf{Proxies: }A proxy sits in between client and server, to simplify the client's requests to servers and to provide anonymity to clients. There are various types of proxies based on what specific task they do in addition to the basic task. The simplest proxies which passes the requests and responses unmodified are also known as \textbf{Gateways}.
\item \textbf{VPN :}A virtual private network establishes a private network across public networks. It allows user to send and receive data across public networks maintaining the policy and security enforced by the private network.  
\item \textbf{Wire: }A simple middlebox which sends packets from its input interface to output interface. It is generally used to give a performance baseline.            
\end{itemize}
\subsection{Application specific middleboxes}

\chapter{Related Work}
\noindent Over the past few years, much research has been done in the area of customization of middlebox functionalities, customization of hypervisors which hosts the middleboxes and customization of operating systems for middleboxes. It is very important to understand the work done so far, before moving ahead with our proposed solution.

Based on our goal, which is to improve the performance of the middleboxes in virtualized data centres, we can classify the techniques studied so far into three categories.
\begin{itemize}
\item Redesign the Operating System
\item Regesign the Hypervisor
\item Redesign the hardware
\end{itemize}
We will briefly discuss some techniques which fall inside these categories. And we will also state how it relates to or contrast with our proposed solution.
\section{Redesign the Operating System}
ClickOS\cite{R1} proposes replacement of traditional guest operating system with some optimized, light weight operating system. Linux as a guest operating system in VMs is actually over provisioning for middlebox applications. Middlebox uses very few operating system services in addition to network connectivity. Traditional Linux operating system takes around 128MB space on guest VM and also takes around 5 second time to boot, that is very slow in context of middlebox setup time. So, they came up with minimalistic operating system for virtual machines. ClickOS virtual machines which runs MiniOS are very small in size that is only around 5 MB and also very fast to setup, only 30 msec boot time. MiniOS has a single address space, no kernel/user space separation and a cooperative scheduler. Basically ClickOS are single core machines. They have used Xen as a hypervisor for experiments. It also improves scalability of the middleboxes. With traditional full fledged OS running on the VMs, number of tenants supported on a physical machine are very small. But with MiniOs running on ClickOS machine the number increases drastically. This technique improves the scalability and the setup time for middleboxes but it does not address redundancy among middlebox functionalities. Though it can create new ClickOS VM quickly for scaling, ClickOS VMs do not have symmetric multiprocessing support.             

OSv\cite{R2} also offeres optimized operating system for middleboxes in cloud environment. VMs in cloud usually runs linux as an OS. When we say middlebox is running on a VM, it often means that only single application (middlebox application) runs on that VM. In this case, managing multiple VMs on hypervisor only means that managing multiple applications on the hypervisor. Hypervisor provides features like isolation among VMs, hardware abstraction, memory management. In this case hypervisor almost act as an OS for middlebox applications. So, when we use traditional operating system to run middlebox applications, the above mentioned features become redundant. It affects overall performance of the host machine. OSv is a guest OS specially designed to run a single application on a VM in the cloud environment. It has very small OS image, dedicates more memory to the application and lesser boot time. OSv supports various hypervisors and processors with minimal change in architecture specific code. For 64-bit x86 processors, it supports KVM, Xen, VMware and Virtual Box hypervisors. It also improves scalability of middleboxes just like ClickOS. But it supports Symmetric multi-processing which is an advantage over ClickOS. It also gives throughput and latency improvement for middleboxes.

Unikernels\cite{R3} also provides light weight machines to deploy middlebox applications in cloud environment. Unikernels are single purpose appliances, they strips away functionalities of general purpose system at compile time. It is inherently suitable for middleboxes. It takes into consideration the idea of library OS, in which an application links against separate OS service libraries and unused services from the library are eliminated from the final image by the compiler. For an example, virtual machine image of a DNS server can be as small as 200KB. Mirage OS[*] is an example of such a library OS. It is written in OCaml, a functional programming language and it runs on the Xen hypervisor[*]. It also improves scalability and setup latency for the middleboxes.                    
\section{Redesign the Hypervisors}
Container\cite{R4} modifies the hypervisor to eliminate redundant features among the hypervisor and the guest OS. Actually it drops the idea of traditional hypervisor in virtualization. It modifies the host operating system to support isolated execution environments for applications while running on the same kernel. It improves resource utilization among guests and lowers per guest overhead. It also improves the overall performance of the system. I/O related workload, server type workload performs better on container based system compared to hypervisor based system. It also scales well compared to hypervisor setup. But still most of the data centres prefer to use hypervior based system. Hypervisor based system can support multiple kernels but by design container based system can not support the same. Container also does not have support for VM migration. Hypervisors are the industry standard for virtualization.       

CoVisor\cite{R5} is a hypervisor, which uses a completely different approach to host middlebox applications in a software defined network. It uses basic concept of network hypervisor that is to manage virtual networks on a physical network. Along with usual middlebox hosting challenges, it deals with some SDN specific challenges as well. For example, middlebox applications used by different vendors may be built by using different SDN APIs. Covisor makes it possible to run any middlebox irrespective of what SDN API is used. Covisor provides facility to assemble multiple middlebox applications as per the administrator configuration. Each middlebox can be used independently, in parallel or sequential with other middlebox, or conditionally. Administrator can provide abstract virtual topology for each middlebox. It restricts each middlebox's view of the physical network. Configuration file provided by administrator includes policies to assemble middleboxes, mapping for each middlebox's virtual network components to actual physical components and access control limitation for each middlebox. Covisor was tested with respect to its composition efiiciency and devirtualization efficiency. It gives satisfactory results on metrics like policy compilation time, Rule updation time and total devirtualization time. Covisor is still in development phase and as most of the data centres use traditional hypervisors, switching to Covisor needs lots of modification to existing data centre architecture. 

We already discussed ClickOS\cite{R1} phase-I in the first section. ClickOS phase-II falls under this section. ClickOS authors ran ClickOS machines on Xen hypervisor for experiments. They modified the Xen hypervisor to achieve throughput and better resource utilization. In traditional Xen hypervisor networking, when a packet is received on physical NIC, it traverses through network driver(dom0), software switch(dom0), virtual interface(dom0), netback driver(dom0) and netfront driver(guest machine) before getting processed by a middlebox. ClickOS technique modifies memory grant mechanism, netback driver, software switch and netfront driver to increase the middlebox throughput. Modified version of Xen is still not capable of handling a chain of middleboxes. Longer the chain is, lower the throughput. Even after all the modifications, hypercalls done by Xen for each packet transmission remains the bottleneck in this case.            
\section{Redesign the hardware}

\chapter{Design of MsH}
\section{MB specific hypervisor module}
\section{Interface between MB and Hypervisor}
\section{MB specific state stored at hypervisor}
\section{Different packet traversal paths}
\section{Cost analysis}
\section{Challenges}

\chapter{Experiments}
\section{Experimentation setup}
\section{Results}
\subsection{Interrupts generated by packets}
\subsection{Time required to process a packet}
\subsection{Throughput of a client application}
\section{Challenges}

\chapter{Conclusion and Future work}

\newpage
\bibliographystyle {acm}
\bibliography {report}
\end{document}

